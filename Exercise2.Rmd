---
title: "Practica1"
author: "Álvaro Ribot & Jose Pérez Cano"
date: "10/02/2020"
output: 
  html_document: default
  pdf_document: default
---


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
border-radius: 5px;
font-style: italic;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(car)
library(MASS)

graphics.off()      # reset/close all graphical devices 
```

# Exercici 2

### a)

```{r}
dd2 <- read.table("http://www-eio.upc.es/~jan/Data/MVA/PovertyStudy.dat", header=T)
head(dd2)
```

### b) Relations between variables

```{r}
pairs(dd2)
```


### c) Missing values
```{r}
dd2$GNP

sprintf("There are %i countries without GNP", sum(dd2$GNP == -99))
```
It seems the missing values are coded with -99.

### d) Substituting by NAs 
```{r}
dd2$GNP[dd2$GNP == -99] <- NA
```

### e) Boxplot & Q-Q plot
```{r}
with(dd2, boxplot(GNP))
with(dd2, qqnorm(GNP))
```

### f) BoxCox 
```{r}
boxcox(lm(GNP~1, dd2),lambda = seq(-1, 1, by=0.1))
```
We can suppose $\lambda=0 \implies GNP^{(\lambda)}_{i}=ln(GNP_i)$


### g) Boxplot of transformed variable
```{r}
GNPmod <- log(dd2$GNP)
boxplot(GNPmod)
```
Now it follows a normal distribution as we can see a symmetric boxplot.


### h) Linear regression 
```{r}
m2 <- lm(GNPmod~Birth+Death+Infant+LifeEM+LifeEF, dd2)
anova(lm(GNPmod~1), m2)
sprintf("The percentage of variance of the data explained by the model is %.2f%%", summary(m2)$r.squared*100)
```

### i) Predicted values for missing values
```{r}
GNPmancant <- dd2[is.na(dd2$GNP),]
predict(m2, newdata = GNPmancant)
```

### j) Variance of residuals
```{r}
aaa <- anova(lm(GNPmod~1),m2)
sprintf("The residual variance is %.2f", resvar <- aaa$RSS[2]/aaa$Df[2])
```

### k) Predictions with gaussian noise
```{r}
set.seed(123)
noise <- rnorm(n = 6, sd = sqrt(resvar))
predict(m2, newdata = GNPmancant) + noise
```


# Exercise 5
## Data 
### a) Read data
```{r}
dd <- read.table("http://www-eio.upc.es/~jan/Data/MVA/kernels.dat", header=T)
head(dd)
```
## First questions
### b) Means
```{r}
apply(dd, MARGIN=2, FUN=mean)
```
### c) Centered dataframe
```{r}
ddc <- scale(dd, scale=FALSE)
head(ddc)
```
### d) Covariance matrix
They aren't comparable because they are in different scales.
```{r}
cov(dd)
sprintf("The variable with more variance is %s", names(which.max(apply(dd, MARGIN=2, FUN=var))))
```
### e) Correlation matrix
```{r}
cor(dd)
scatterplotMatrix(dd)
abs(cor(dd)) > 0.5 # Strong linear correlation
```
### f) Standardized data frame
```{r}
dds <- scale(dd)
head(dds)
```
### g)
We observe it is equal to the correlation matrix of the original dataframe.
```{r}
cov(dds)
```
### h) Euclidean distance
```{r}
as.matrix(dist(dd[1:5,]))
```

### i) Centered / Standardize euclidean distance
```{r}
print("Centered")
as.matrix(dist(ddc[1:5,]))

print("Standardized")
as.matrix(dist(dds[1:5,]))
```

### j) 
The transformation in question is
$f(\bf{x}) = \frac{\bf{x}-\overline{x}}{Var(\bf{x})}+\overline{x}$
where $\overline{x}$ is the vector where all entries are the mean of the vector x.
```{r}
ddn <- t(t(dds)+apply(dd, MARGIN=2, FUN=mean))
head(ddn)
apply(ddn, MARGIN=2, FUN=mean)
abs(apply(ddn, MARGIN=2, FUN=mean)-apply(dd, MARGIN=2, FUN=mean)) < 1e-7
apply(ddn, MARGIN=2, FUN=var)
```






